---
layout: post
title: "Bayesian Inference: Beta-Binomial Distribution"
author: Kelsey Moore
description: Tutorial of how to create a beta-binomial posterior distribution
image: /assets/images/betabinomialheader.jpg
---

## What is Bayesian Inference?

In traditional statistical analysis, we draw conclusions based solely on sample data. Unfortunately, this limits our ability to make definitive inference. For example, we make confidence intervals, saying that we are some percent "confident" that the population parameter is within the determined bounds - we cannot say that there is some "probability" that the population parameter is within those bounds. Bayesian inference allows us to speak of population parameters in terms of probability, saying, for example, that there is some percent chance that the population parameter is within the determined bounds.

## Components of Bayesian Inference

Performing Bayesian inference involves four distributions: the prior distribution of the desired parameter, the likelihood of the collected data given the parameters, the marginal likelihood of the data, and the posterior distribution. They relate to each other as follows:

$$Posterior = \frac{Likelihood \times Prior}{MarginalLikelihood} \rightarrow \pi(\theta|y) = \frac{f(y|\theta)\pi(\theta)}{f(y)}$$

## Eliciting a Prior Distribution

One of the most challenging parts of Bayesian inference is determining what the prior distribution should be. Since we are trying to determine parameters of the population, it would seem that we have to pull the prior distribution out of thin air. However, in most cases, we have at least some prior knowledge of the parameter we are interested in. For example, all though we may not know the exact proportion of students at BYU who are from Utah, we at least know that 10% would be a pretty inaccurate guess. 

Once you have used your previous knowledge to elicit a prior distribution, you should check to make sure that the distribution actually conveys the information you think it does. In order to do this, you can use something called a prior-predictive distribution. 

$$f(y_{new}) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \displaystyle{n_{new} \choose y_{new}} \frac{\Gamma(a+y_{new})\Gamma(b+n_{new}-y_{new})}{\Gamma(a+b+n_{new})} \times \mathbb{1}_{y_{new} \in \{0,1,...,n_{new}\}}$$

This process simulates taking a number of samples $(n)$ and determines the probability of any given number of successes $(0:n)$, given the prior distribution. 

Consider an example where we think our prior distribution should be $Beta(7,3)$. This means that we think there is a 70% chance of success. The prior-predictive distribution, with a sample size of 20, would look like this:

<p align="center">
  <img src="https://raw.githubusercontent.com/kbmoore02/my386blog/main/assets/images/pp73.jpg" alt="" style="width:600px;"/>
</p>

This graph shows that the most likely number of successes is 14 out of 20, which is 70%. However, this still puts pretty significant probability on any number of successes between 7 and 20. Perhaps based on our background knowledge this doesn't seem entirely appropriate. So, we can adjust our prior distribution to $Beta(70,30)$ and check the prior-predictive distribution again. Now it looks like this:

<p align="center">
  <img src="https://raw.githubusercontent.com/kbmoore02/my386blog/main/assets/images/pp7030.jpg" alt="" style="width:600px;"/>
</p>

This looks more like what we are thinking, so we can proceed with our inference using a $Beta(70,30)$ prior distribution.

## Bayesian Inference with a Beta-Binomial Distribution

The beta distribution is a conjugate prior for $\theta$ when the binomial likelihood is assumed. This means that a binomial likelihood for y $\| \theta$ coupled with a beta prior distribution for $\theta$ produces a beta posterior distribution for $\theta \|$ y.

## Example

Let's consider an example where we are interested in the proportion of people in the United States who are multi-lingual. We create a prior distribution of $\theta \sim Beta(a=10,b=40)$, suggesting that 20% of US residents are multi-lingual. The distribution would look like this:

<p align="center">
  <img src="https://raw.githubusercontent.com/kbmoore02/my386blog/main/assets/images/prior.jpg" alt="" style="width:600px;"/>
</p>

After we collected 100 samples, 24 people were multi-lingual (successes) and 76 people were not (failures). Creating the posterior distribution for beta-binomial is very simple. It follows this formula: $Beta(a* = a + successes,$ $b* = b + failures)$. So for this example, the posterior distribution is $Beta(34,116)$, shown in red:

<p align="center">
  <img src="https://raw.githubusercontent.com/kbmoore02/my386blog/main/assets/images/posterior.jpg" alt="" style="width:600px;"/>
</p>

Now that we've created our posterior distribution, we can draw inferences on the true population proportion of multi-lingual people. Using R, we can compute a credible interval. 

$$qbeta(c(0.025,0.975), 34, 116)$$

There is a 95% probability that the true proportion of multi-lingual people amongst all people in the US is between 0.16 and 0.30. 

The $a$ and $b$ parameters in the Beta distribution can be thought of as $a=successes$ and $b=failures$. So the prior distribution represents 10 successes out of 50 (a proportion of 0.2), and the data has 24 successes out of 100 (a proportion of 0.24). After combining the two, the "overall" number of successes is 34 out of 150 (a proportion of 0.23), so the posterior distribution is pulled slightly higher than the prior distribution.

## Another Example

In the previous example, the proportion of successes from the data gathered was similar to the proportion of successes in the prior distribution. Consider an example where instead, the data is wildly different than the prior distribution.

Let's say we are interested in the proportion of BYU students who enjoy playing spikeball. The prior distribution we chose for this situation is $Beta(8,2)$. We gather a sample of 5 students, and only 1 person liked spikeball (successes), while 4 people did not (failures). After accounting for the data, our posterior distribution is $Beta(9,6)$. 

<p align="center">
  <img src="https://raw.githubusercontent.com/kbmoore02/my386blog/main/assets/images/second_example_1.jpg" alt="" style="width:600px;"/>
</p>

As we can see, the posterior distribution peaks at a lower proportion than the prior distribution, because the data had a lower proportion than the prior distribution. However, it is not pulled down nearly as far as we would hope, considering that the sample proportion was so low (0.2 compared to the posterior distribution proportion of 0.6). The reason that this happens is because the sample size is so low. If instead, we took 50 samples, with 10 successes and 40 failures (still a proportion of 0.2), the distribution would look like this:

<p align="center">
  <img src="https://raw.githubusercontent.com/kbmoore02/my386blog/main/assets/images/second_example_2.jpg" alt="" style="width:600px;"/>
</p>

Now the posterior distribution proportion is 0.3, much closer to what the data tells us. Because the sample size is larger, it has more sway over the posterior distribution than the prior distribution. Therefore, it is ideal to have a large sample size when performing Bayesian analysis. You might ask why we don't use only the data if we care more about what the data has to say than about anything else, but as stated at the beginning of this article, without using the prior distribution to perform Bayesian analysis, we cannot speak about population parameters in terms of probability, only in terms of confidence. 
