---
layout: post
title: "Bayesian Inference: Beta-Binomial Distribution"
author: Kelsey Moore
description: Tutorial of how to create a beta-binomial posterior distribution
image: 
---

## What is Bayesian Inference?

In traditional statistical analysis, we draw conclusions based solely on sample data. Unfortunately, this limits our ability to make definitive inference. For example, we make confidence intervals, saying that we are some percent "confident" that the population parameter is within the determined bounds - we cannot say that there is some "probability" that the population parameter is within those bounds. Bayesian inference allows us to speak of population parameters in terms of probability, saying, for example, that there is some percent chance that the population parameter is within the determined bounds.

## Components of Bayesian Inference

Performing Bayesian inference involves four distributions: the prior distribution of the desired parameter, the likelihood of the collected data given the parameters, the marginal likelihood of the data, and the posterior distribution. They relate to each other as follows:

```math
Posterior = \frac{Likelihood \times Prior}{MarginalLikelihood} \rightarrow \pi(\theta|y) = \frac{f(y|\theta)\pi(\theta)}{f(y)}
```

## Eliciting a Prior Distribution

One of the most challenging parts of Bayesian inference is determining what the prior distribution can be. Since we are trying to determine parameters of the population, it would seem that we have to pull the prior distribution out of thin air. However, in most cases, we have at least some prior knowledge of the parameter we are interested in. For example, all though we may not know the exact proportion of students at BYU who are from Utah, we at least know that 10% would be a pretty inaccurate guess. 

Once you have used your previous knowledge to elicit a prior distribution, you should check to make sure that the distribution actually conveys the information you think it does. In order to do this, you can use something called a prior-predictive distribution. 

```math
f(y_{new}) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \displaystyle{n_{new} \choose y_{new}} \frac{\Gamma(a+y_{new})\Gamma(b+n_{new}-y_{new})}{\Gamma(a+b+n_{new})} \times \mathbb{1}_{y_{new} \in \{0,1,...,n_{new}\}}
```

## Bayesian Inference with a Beta-Binomial Distribution

The beta distribution is a conjugate prior for $\theta$ when the binomial likelihood is assumed. This means that a binomial likelihood for y | $\theta$ coupled with a beta prior distribution for $\theta$ produces a beta posterior distribution for $\theta$ | y.

## Example

Let's consider an example where we are interested in the proportion of people who consider themselves to be "morning people." We create a prior distribution of $\theta$~Beta(a=3,b=7), suggesting that 30% of people are morning people. The distribution would look like this:

![Figure](https://raw.githubusercontent.com/kbmoore02/my386blog/main/assets/images/prior.jpg)

After we collected 30 samples, 13 people identified themselves as morning people (successes) and 17 people did not (failures). Creating the posterior distribution for beta-binomial is very simple. It follows this formula: Beta(a* = a + successes, b* = b + failures). So for this example, the posterior distribution is Beta(16, 24):

![Figure](https://raw.githubusercontent.com/kbmoore02/my386blog/main/assets/images/posterior.jpg)

Now that we've created our posterior distribution, we can draw inferences on the true population proportion of morning people. Using R, we can compute a credible interval. 

```math
qbeta(c(0.025,0.975), 16, 24)
```

There is a 95% probability that the true proportion of morning-people amongst all people is between 0.26 and 0.55. 

Add about how the graph changes. show another example where the data is way different.
